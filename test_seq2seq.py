"""
ä¼˜åŒ–ç‰ˆæµ‹è¯•è„šæœ¬
æ”¹è¿›ï¼š
1. ä½¿ç”¨å®Œæ•´å¥å­ä½œä¸ºæ–‡ä»¶åï¼ˆç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿ï¼‰
2. å½©è‰²éª¨æ¶æ¸²æŸ“ï¼ˆå¤´éƒ¨çº¢è‰²ï¼Œæ‰‹è‡‚ç»¿è‰²ï¼Œé¢ˆéƒ¨è“è‰²ï¼‰
ä½¿ç”¨æ–¹æ³•: python enhanced_test_optimized.py
"""

import numpy as np
import torch
import scipy.io as scio
from model.seq2seq_structure import Seq2SeqModel
from model.seq2seq_tester import Seq2SeqTester
from utils.my_functions import load_w2v
from model.visualization_original import visualize_action_complete
import os


def test_with_enhanced_visualization(tester, w2v_model, sentence, output_dir='./test_results'):
    """
    ä½¿ç”¨å¢å¼ºå¯è§†åŒ–æµ‹è¯•æ¨¡å‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    Args:
        tester: Seq2SeqTesterå¯¹è±¡
        w2v_model: Word2Vecæ¨¡å‹
        sentence: æµ‹è¯•å¥å­
        output_dir: è¾“å‡ºç›®å½•
    """
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•å¥å­: '{sentence}'")
    print(f"{'='*70}")

    # 1. è½¬æ¢å¥å­ä¸ºembedding
    print("\n[1/4] ç¼–ç å¥å­...")
    words = sentence.lower().split()
    test_script = np.zeros((1, 300, 30))

    found_words = 0
    for i, word in enumerate(words[:30]):
        if word in w2v_model:
            test_script[0, :, i] = w2v_model[word]
            print(f"  âœ“ '{word}'")
            found_words += 1
        else:
            print(f"  âš ï¸  '{word}' (ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œä½¿ç”¨é›¶å‘é‡)")

    test_script_len = np.array([min(len(words), 30)])
    print(f"  ç¼–ç å®Œæˆ: {found_words}/{len(words)} ä¸ªè¯åœ¨è¯æ±‡è¡¨ä¸­")

    # 2. ç”ŸæˆåŠ¨ä½œ
    print(f"\n[2/4] ç”ŸæˆåŠ¨ä½œåºåˆ—...")
    generated_action = tester.test(test_script, test_script_len)

    print(f"  âœ“ åŠ¨ä½œå½¢çŠ¶: {generated_action.shape}")
    print(f"  âœ“ å‡å€¼: {generated_action.mean():.4f}")
    print(f"  âœ“ æ ‡å‡†å·®: {generated_action.std():.4f}")
    print(f"  âœ“ èŒƒå›´: [{generated_action.min():.4f}, {generated_action.max():.4f}]")

    # 3. ä¿å­˜.npyæ–‡ä»¶ï¼ˆä½¿ç”¨å®Œæ•´å¥å­åï¼‰
    print(f"\n[3/4] ä¿å­˜åŠ¨ä½œæ•°æ®...")
    os.makedirs(output_dir, exist_ok=True)

    # ä½¿ç”¨å®Œæ•´å¥å­ï¼Œæ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    filename_prefix = sentence.lower().replace(' ', '_')
    npy_path = os.path.join(output_dir, f'{filename_prefix}_action.npy')

    np.save(npy_path, generated_action)
    print(f"  âœ“ åŠ¨ä½œæ•°æ®ä¿å­˜åˆ°: {npy_path}")

    # 4. ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–ï¼ˆå½©è‰²ç‰ˆï¼‰
    print(f"\n[4/4] ç”Ÿæˆå½©è‰²å¯è§†åŒ–...")
    files = visualize_action_complete(generated_action, sentence, output_dir)

    return generated_action, files


def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ä¼˜åŒ–ç‰ˆSeq2Seqæµ‹è¯• - å®Œæ•´æ–‡ä»¶å + å½©è‰²éª¨æ¶")
    print("="*70)
    print("\næ”¹è¿›ç‚¹:")
    print("  âœ¨ æ–‡ä»¶åä½¿ç”¨å®Œæ•´å¥å­ï¼ˆç”¨ä¸‹åˆ’çº¿è¿æ¥ï¼‰")
    print("  ğŸ¨ å¤´éƒ¨å…³é”®ç‚¹: çº¢è‰²")
    print("  ğŸ¨ æ‰‹è‡‚/æ‰‹éƒ¨å…³é”®ç‚¹: ç»¿è‰²")
    print("  ğŸ¨ é¢ˆéƒ¨å…³é”®ç‚¹: è“è‰²")

    # ==================== é…ç½® ====================
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")

    # æ¨¡å‹å‚æ•°
    sentence_steps = 30
    action_steps = 32
    dim_sentence = 300
    dim_char_enc = 300
    dim_gen = 300
    dim_random = 10

    # è·¯å¾„
    model_path = './seq2seq_model/model_epoch_500.pth'
    mean_pose_path = './data/mean_pose.mat'
    w2v_path = './data/GoogleNews-vectors-negative300.bin'
    output_dir = './test_results'

    # ==================== æ£€æŸ¥æ–‡ä»¶ ====================
    print("\næ£€æŸ¥å¿…è¦æ–‡ä»¶...")
    required_files = [
        (model_path, "è®­ç»ƒå¥½çš„æ¨¡å‹"),
        (mean_pose_path, "å¹³å‡pose"),
        (w2v_path, "Word2Vecæ¨¡å‹")
    ]

    for file_path, description in required_files:
        if os.path.exists(file_path):
            print(f"  âœ“ {description}: {file_path}")
        else:
            print(f"  âœ— {description}æœªæ‰¾åˆ°: {file_path}")
            print(f"\nè¯·ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨:")
            print(f"  - {model_path}")
            print(f"  - {mean_pose_path}")
            print(f"  - {w2v_path}")
            return

    # ==================== åŠ è½½æ¨¡å‹ ====================
    print("\n" + "="*70)
    print("åŠ è½½æ¨¡å‹å’ŒWord2Vec")
    print("="*70)

    print("\nåŠ è½½åˆå§‹poseä»¥æ¨æ–­åŠ¨ä½œç»´åº¦...")
    init_pose = scio.loadmat(mean_pose_path)['mean_vector']
    dim_action = init_pose.shape[0]  # ä»åˆå§‹poseæ¨æ–­åŠ¨ä½œç»´åº¦
    print(f"åŠ¨ä½œç»´åº¦: {dim_action}ç»´ (ä»mean_pose.matæ¨æ–­)")

    print("\nåˆ›å»ºæ¨¡å‹ç»“æ„...")
    model = Seq2SeqModel(
        sentence_steps=sentence_steps,
        action_steps=action_steps,
        dim_sentence=dim_sentence,
        dim_char_enc=dim_char_enc,
        dim_gen=dim_gen,
        dim_action=dim_action,  # ä»æ•°æ®æ¨æ–­çš„åŠ¨ä½œç»´åº¦
        dim_random=dim_random
    )

    print("åˆ›å»ºæµ‹è¯•å™¨...")
    tester = Seq2SeqTester(
        model=model,
        init_pose=init_pose,
        model_path=model_path,
        sentence_steps=sentence_steps,
        action_steps=action_steps,
        dim_sentence=dim_sentence,
        dim_char_enc=dim_char_enc,
        dim_gen=dim_gen,
        dim_action=dim_action,
        dim_random=dim_random,
        device=device
    )

    print("åŠ è½½Word2Vecæ¨¡å‹...")
    w2v_model = load_w2v(w2v_path)

    # ==================== æµ‹è¯•å¥å­ ====================
    print("\n" + "="*70)
    print("å¼€å§‹æµ‹è¯•")
    print("="*70)

    # å®šä¹‰æµ‹è¯•å¥å­ï¼ˆå¯ä»¥æ˜¯æ›´é•¿çš„æè¿°ï¼‰
    test_sentences = [
        "a woman is dancing",
        "a man is lifting weights",
        "a person is waving hands",
        "someone is throwing a ball",
        "a girl is jumping",
        "a man playing drums"
    ]

    print(f"\nå°†æµ‹è¯• {len(test_sentences)} ä¸ªå¥å­")
    print("\næ¯ä¸ªå¥å­å°†ç”Ÿæˆ 4 ä¸ªæ–‡ä»¶:")
    print("  1. [å®Œæ•´å¥å­]_8frames.png        - 8ä¸ªå…³é”®å¸§ï¼ˆå½©è‰²ï¼‰")
    print("  2. [å®Œæ•´å¥å­]_animation.gif      - 32å¸§åŠ¨ç”»ï¼ˆå½©è‰²ï¼‰")
    print("  3. [å®Œæ•´å¥å­]_32frames_grid.png - 32å¸§ç½‘æ ¼å›¾ï¼ˆå½©è‰²ï¼‰")
    print("  4. [å®Œæ•´å¥å­]_action.npy        - åŠ¨ä½œæ•°æ®")

    print("\næ–‡ä»¶åç¤ºä¾‹:")
    for sent in test_sentences[:1]:
        example_name = sent.lower().replace(' ', '_')
        print(f"  '{sent}'")
        print(f"    â†’ {example_name}_8frames.png")
        print(f"    â†’ {example_name}_animation.gif")
        print(f"    â†’ {example_name}_32frames_grid.png")
        print(f"    â†’ {example_name}_action.npy")

    # æµ‹è¯•æ¯ä¸ªå¥å­
    all_results = {}
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\n\n{'#'*70}")
        print(f"æµ‹è¯• {i}/{len(test_sentences)}")
        print(f"{'#'*70}")

        try:
            action, files = test_with_enhanced_visualization(
                tester, w2v_model, sentence, output_dir
            )
            all_results[sentence] = {'action': action, 'files': files}
        except Exception as e:
            print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue

    # ==================== æ€»ç»“ ====================
    print("\n" + "="*70)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*70)

    print(f"\nâœ“ æˆåŠŸç”Ÿæˆ {len(all_results)} ä¸ªåŠ¨ä½œåºåˆ—")
    print(f"âœ“ ç»“æœä¿å­˜åœ¨: {output_dir}")

    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    total_files = 0
    for sentence, result in all_results.items():
        print(f"\n  '{sentence}':")
        for file_type, path in result['files'].items():
            if os.path.exists(path):
                size_kb = os.path.getsize(path) / 1024
                filename = os.path.basename(path)
                print(f"    âœ“ {filename} ({size_kb:.1f} KB)")
                total_files += 1
            else:
                print(f"    âœ— {file_type}: æœªç”Ÿæˆ")
        # æ˜¾ç¤º.npyæ–‡ä»¶
        npy_name = sentence.lower().replace(' ', '_') + '_action.npy'
        npy_path = os.path.join(output_dir, npy_name)
        if os.path.exists(npy_path):
            size_kb = os.path.getsize(npy_path) / 1024
            print(f"    âœ“ {npy_name} ({size_kb:.1f} KB)")
            total_files += 1

    print(f"\næ€»å…±ç”Ÿæˆ: {total_files} ä¸ªæ–‡ä»¶")

    print("\n" + "="*70)
    print("æ–‡ä»¶åä¼˜åŒ–è¯´æ˜")
    print("="*70)
    print("âœ¨ æ—§ç‰ˆæ–‡ä»¶å (åªç”¨å‰3ä¸ªè¯):")
    print("   a_woman_is_8frames.png")
    print("   a_woman_is_animation.gif")
    print("\nâœ¨ æ–°ç‰ˆæ–‡ä»¶å (ä½¿ç”¨å®Œæ•´å¥å­):")
    print("   a_woman_is_dancing_gracefully_8frames.png")
    print("   a_woman_is_dancing_gracefully_animation.gif")
    print("\nä¼˜ç‚¹:")
    print("  âœ“ æ–‡ä»¶åæ›´å…·æè¿°æ€§")
    print("  âœ“ é¿å…ä¸åŒå¥å­æ–‡ä»¶åå†²çª")
    print("  âœ“ æ›´å®¹æ˜“ç®¡ç†å’ŒæŸ¥æ‰¾")

    print("\n" + "="*70)
    print("é¢œè‰²ç¼–ç è¯´æ˜")
    print("="*70)
    print("ğŸ”´ çº¢è‰²: å¤´éƒ¨å…³é”®ç‚¹ (joint 7)")
    print("ğŸŸ¢ ç»¿è‰²: æ‰‹è‡‚å’Œæ‰‹éƒ¨å…³é”®ç‚¹ (joints 1-6)")
    print("ğŸ”µ è“è‰²: é¢ˆéƒ¨å…³é”®ç‚¹ (joint 0, åŸºå‡†ç‚¹)")
    print("\nä¼˜ç‚¹:")
    print("  âœ“ æ›´å®¹æ˜“è¯†åˆ«ä¸åŒèº«ä½“éƒ¨ä½")
    print("  âœ“ è§†è§‰ä¸Šæ›´ç›´è§‚")
    print("  âœ“ ä¾¿äºåˆ†æåŠ¨ä½œè´¨é‡")

    print("\n" + "="*70)
    print("ä½¿ç”¨å»ºè®®")
    print("="*70)
    print("1. æŸ¥çœ‹ *_8frames.png å¿«é€Ÿé¢„è§ˆåŠ¨ä½œ")
    print("2. æ’­æ”¾ *_animation.gif è§‚å¯Ÿæµç•…åº¦")
    print("3. æ£€æŸ¥ *_32frames_grid.png åˆ†æç»†èŠ‚")
    print("4. æ³¨æ„è§‚å¯Ÿ:")
    print("   - çº¢è‰²å¤´éƒ¨æ˜¯å¦ç§»åŠ¨åˆç†")
    print("   - ç»¿è‰²æ‰‹è‡‚åŠ¨ä½œæ˜¯å¦è‡ªç„¶")
    print("   - è“è‰²é¢ˆéƒ¨ä½œä¸ºç¨³å®šçš„å‚è€ƒç‚¹")
    print("\n5. å¦‚æœæ•ˆæœå¥½ï¼Œå¯ä»¥å¼€å§‹GANè®­ç»ƒï¼")


if __name__ == "__main__":
    main()
